"""
LangGraph Multi-Agent åœ˜éšŠ

ä½¿ç”¨ LangGraph å¯¦ä½œæ³•è¦æŸ¥è©¢çš„å¤š Agent å”ä½œç³»çµ±ï¼š
- Planner: åˆ†ææŸ¥è©¢ä¸¦è¦åŠƒæœå°‹ç­–ç•¥
- Researcher: åŸ·è¡Œæœå°‹å’Œè³‡æ–™æ”¶é›†
- Validator: é©—è­‰çµæœçš„æº–ç¢ºæ€§
"""

import json
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Callable, Generator, Literal, Optional, TypedDict

from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_openai import AzureChatOpenAI
from langgraph.graph import END, StateGraph

from ..database import BaselineManager
from ..utils.config import load_prompt
from .tool_executor import execute_tool_call, parse_tool_results
from .tool_schemas import get_tool_schemas
from .tools import fetch_pdf_content, fetch_webpage

load_dotenv()


# ===== State Definition =====
class AgentState(TypedDict):
    """Agent ç‹€æ…‹å®šç¾©"""
    messages: list  # å…§éƒ¨ Agent é€šè¨Šæ—¥èªŒ
    query: str  # åŸå§‹æŸ¥è©¢
    jurisdiction: str  # ç›®æ¨™åœ°å€
    conversation_history: str  # å¤šè¼ªå°è©±æ­·å²ï¼ˆæ ¼å¼åŒ–å­—ä¸²ï¼‰
    previous_results_summary: str  # ä¸Šæ¬¡æŸ¥è©¢çµæœæ‘˜è¦
    planner_analysis: dict  # Planner åˆ†æçµæœ
    clarification_needed: bool  # æ˜¯å¦éœ€è¦æ¾„æ¸…
    questions: list  # æ¾„æ¸…å•é¡Œ
    search_results: list  # æœå°‹çµæœ
    validated_results: list  # é©—è­‰å¾Œçš„çµæœ
    status: str  # ç•¶å‰ç‹€æ…‹
    error: Optional[str]  # éŒ¯èª¤è¨Šæ¯


# ===== LLM Configuration (Thread-Safe Singleton) =====
_llm_instance: Optional[AzureChatOpenAI] = None
_llm_lock = threading.Lock()


def get_llm() -> AzureChatOpenAI:
    """
    å–å¾— LLM å¯¦ä¾‹ (Azure OpenAI) - ç·šç¨‹å®‰å…¨å–®ä¾‹æ¨¡å¼

    ä½¿ç”¨ Double-Checked Locking ç¢ºä¿ç·šç¨‹å®‰å…¨ï¼Œ
    åŒæ™‚é¿å…æ¯æ¬¡å‘¼å«éƒ½éœ€è¦ç²å–é–çš„æ•ˆèƒ½é–‹éŠ·ã€‚
    """
    global _llm_instance

    # ç¬¬ä¸€æ¬¡æª¢æŸ¥ï¼ˆç„¡é–ï¼‰
    if _llm_instance is not None:
        return _llm_instance

    # ç²å–é–å¾Œå†æ¬¡æª¢æŸ¥
    with _llm_lock:
        if _llm_instance is not None:
            return _llm_instance

        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        azure_key = os.getenv("AZURE_OPENAI_API_KEY")
        azure_deployment = os.getenv("AZURE_OPENAI_GPT5_DEPLOYMENT", "gpt-5.1")
        azure_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")

        if not azure_endpoint or not azure_key:
            raise ValueError("æœªè¨­å®š Azure OpenAI é…ç½®")

        # GPT-5.1: 400K context (272K input, 128K output)
        # ä¸æ”¯æ´è‡ªè¨‚ temperatureï¼Œåªèƒ½ä½¿ç”¨é è¨­å€¼ 1.0
        _llm_instance = AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_key,
            azure_deployment=azure_deployment,
            api_version=azure_version,
            max_tokens=32768,  # 32K tokens è¼¸å‡ºï¼Œè¶³å¤ å®¹ç´å¤§é‡æ³•è¦çš„ JSON
        )

        return _llm_instance


def reset_llm():
    """é‡ç½® LLM å¯¦ä¾‹ï¼ˆç”¨æ–¼æ¸¬è©¦æˆ–é‡æ–°è¼‰å…¥é…ç½®ï¼‰"""
    global _llm_instance
    with _llm_lock:
        _llm_instance = None


# ===== Agent Nodes =====
def planner_node(state: AgentState) -> AgentState:
    """
    Planner Agent: åˆ†ææŸ¥è©¢æ„åœ–ä¸¦åˆ¶å®šæœå°‹ç­–ç•¥
    ç¾åœ¨æ”¯æ´å¤šè¼ªå°è©±æ­·å²ï¼Œèƒ½ç†è§£è¿½å•é¡å‹çš„æŸ¥è©¢
    """
    llm = get_llm()

    # è¼‰å…¥å¤–éƒ¨ prompt
    system_prompt = load_prompt("langgraph_planner")

    # å»ºæ§‹åŒ…å«å°è©±æ­·å²çš„ user_message
    user_message_parts = []

    # åŠ å…¥å°è©±æ­·å²ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    conversation_history = state.get('conversation_history', '')
    if conversation_history:
        user_message_parts.append(f"""ã€å°è©±æ­·å²ã€‘
ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å°è©±è¨˜éŒ„ï¼Œè«‹åƒè€ƒé€™äº›ä¸Šä¸‹æ–‡ä¾†ç†è§£ç•¶å‰æŸ¥è©¢ï¼š
{conversation_history}
""")

    # åŠ å…¥ä¸Šæ¬¡çµæœæ‘˜è¦ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    previous_results = state.get('previous_results_summary', '')
    if previous_results:
        user_message_parts.append(f"""ã€ä¸Šæ¬¡æŸ¥è©¢çµæœæ‘˜è¦ã€‘
{previous_results}
""")

    # ç•¶å‰æŸ¥è©¢
    user_message_parts.append(f"""ã€ç•¶å‰æŸ¥è©¢ã€‘
ä½¿ç”¨è€…æŸ¥è©¢: {state['query']}
æŒ‡å®šåœ°å€: {state['jurisdiction']}
æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è«‹åˆ†æé€™å€‹æŸ¥è©¢ä¸¦æä¾›è¦åŠƒã€‚

**æ³¨æ„**ï¼š
1. å¦‚æœé€™æ˜¯è¿½å•é¡å‹çš„æŸ¥è©¢ï¼ˆå¦‚ã€Œé‚„æœ‰å…¶ä»–å—ï¼Ÿã€ã€Œè«‹è©³ç´°èªªæ˜ç¬¬ä¸€å€‹ã€ï¼‰ï¼Œè«‹æ ¹æ“šå°è©±æ­·å²ç†è§£ç”¨æˆ¶çœŸæ­£æƒ³è¦çš„è³‡è¨Š
2. å°æ–¼è¿½å•ï¼Œåˆ¤æ–·æ˜¯å¦éœ€è¦æ–°æœå°‹ï¼Œé‚„æ˜¯å¯ä»¥åŸºæ–¼ä¹‹å‰çš„çµæœé€²è¡Œè£œå……
3. å¦‚æœç„¡æ³•å¾æ­·å²ä¸­åˆ¤æ–·ç”¨æˆ¶æ„åœ–ï¼Œè«‹è¨­å®š clarification_needed: true""")

    user_message = "\n".join(user_message_parts)

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message),
    ]

    try:
        response = llm.invoke(messages)
        content = response.content

        # è§£æ JSON
        if "```json" in content:
            json_str = content.split("```json")[1].split("```")[0]
            analysis = json.loads(json_str)
        else:
            analysis = json.loads(content)

        # æ›´æ–°ç‹€æ…‹
        state["planner_analysis"] = analysis
        state["clarification_needed"] = analysis.get("clarification_needed", False)
        state["questions"] = analysis.get("questions", [])
        state["messages"].append(AIMessage(content=f"Planner åˆ†æ: {json.dumps(analysis, ensure_ascii=False)}"))

        if state["clarification_needed"]:
            state["status"] = "needs_clarification"
        else:
            state["status"] = "ready_to_search"

    except Exception as e:
        state["error"] = f"Planner éŒ¯èª¤: {str(e)}"
        state["status"] = "error"

    return state


def _get_mandatory_keywords_from_db(region: str, industry: str = None, topic: str = None) -> list[dict]:
    """
    å¾è³‡æ–™åº«å–å¾—å¿…æœé—œéµå­—æ¸…å–®

    Args:
        region: åœ°å€åç¨±ï¼ˆä¸­æ–‡ï¼‰
        industry: ç”¢æ¥­åˆ¥
        topic: ä¸»é¡Œ

    Returns:
        å¿…æœé—œéµå­—æ¸…å–®
    """
    # åœ°å€åç¨±å°æ‡‰åˆ°åœ‹å®¶ä»£ç¢¼
    REGION_TO_CODE = {
        "å°ç£": "TW", "æ—¥æœ¬": "JP", "éŸ“åœ‹": "KR", "ä¸­åœ‹": "CN",
        "é¦™æ¸¯": "HK", "æ–°åŠ å¡": "SG", "é¦¬ä¾†è¥¿äº": "MY", "æ³°åœ‹": "TH",
        "å°å°¼": "ID", "è¶Šå—": "VN", "è²å¾‹è³“": "PH", "å°åº¦": "IN",
        "é˜¿è¯é…‹": "AE", "æ²™çƒåœ°é˜¿æ‹‰ä¼¯": "SA", "ä»¥è‰²åˆ—": "IL", "åœŸè€³å…¶": "TR",
        "æ­ç›Ÿ": "EU", "è‹±åœ‹": "GB", "å¾·åœ‹": "DE", "æ³•åœ‹": "FR",
        "ç¾©å¤§åˆ©": "IT", "è¥¿ç­ç‰™": "ES", "è·è˜­": "NL", "ç‘å£«": "CH",
        "ç‘å…¸": "SE", "æ³¢è˜­": "PL", "ä¿„ç¾…æ–¯": "RU",
        "ç¾åœ‹": "US", "åŠ æ‹¿å¤§": "CA", "å¢¨è¥¿å“¥": "MX",
        "å·´è¥¿": "BR", "é˜¿æ ¹å»·": "AR", "æ™ºåˆ©": "CL", "å“¥å€«æ¯”äº": "CO",
        "æ¾³æ´²": "AU", "ç´è¥¿è˜­": "NZ",
        "å—é": "ZA", "å¥ˆåŠåˆ©äº": "NG", "è‚¯äº": "KE", "åŸƒåŠ": "EG",
    }

    # ç”¢æ¥­åç¨±å°æ‡‰åˆ°ä»£ç¢¼
    INDUSTRY_TO_CODE = {
        "é‡‘èæ¥­": "finance_general", "é‡‘è": "finance_general",
        "éŠ€è¡Œæ¥­": "banking", "éŠ€è¡Œ": "banking",
        "è­‰åˆ¸æ¥­": "securities", "è­‰åˆ¸": "securities",
        "ä¿éšªæ¥­": "insurance", "ä¿éšª": "insurance",
        "é†«ç™‚æ¥­": "healthcare", "é†«ç™‚": "healthcare",
        "ç§‘æŠ€æ¥­": "technology", "ç§‘æŠ€": "technology",
        "é›»ä¿¡æ¥­": "telecom", "é›»ä¿¡": "telecom",
    }

    # ä¸»é¡Œå°æ‡‰åˆ°ä»£ç¢¼
    TOPIC_TO_CODE = {
        "è³‡å®‰": "cybersecurity", "è³‡è¨Šå®‰å…¨": "cybersecurity", "ç¶²è·¯å®‰å…¨": "cybersecurity",
        "å€‹è³‡": "privacy", "å€‹äººè³‡æ–™": "privacy", "éš±ç§": "privacy",
        "åæ´—éŒ¢": "aml", "æ´—éŒ¢é˜²åˆ¶": "aml",
    }

    country_code = REGION_TO_CODE.get(region)
    if not country_code:
        return []

    industry_code = INDUSTRY_TO_CODE.get(industry) if industry else None
    topic_code = TOPIC_TO_CODE.get(topic) if topic else None

    try:
        manager = BaselineManager()
        keywords = manager.get_search_keywords(
            country_code=country_code,
            industry_code=industry_code,
            topic_code=topic_code,
        )
        manager.close()
        return keywords
    except Exception as e:
        print(f"[Researcher] å¾è³‡æ–™åº«å–å¾—å¿…æœæ¸…å–®å¤±æ•—: {e}")
        return []


def researcher_node(state: AgentState) -> AgentState:
    """
    Researcher Agent: è‡ªä¸»æœå°‹åŸ·è¡Œ

    æ ¹æ“š Planner çš„ understood è³‡è¨Šï¼Œè‡ªä¸»åˆ¶å®šä¸¦åŸ·è¡Œæœå°‹ç­–ç•¥ã€‚
    ç©©å®šæ€§ä¾†è‡ªè³‡æ–™åº«ä¸­çš„ã€Œå¿…æœæ¸…å–®ã€+ Prompt æŒ‡å°ã€‚
    """
    if state.get("status") != "ready_to_search":
        return state

    llm = get_llm()
    planner_analysis = state.get("planner_analysis", {})
    understood = planner_analysis.get("understood", {})
    findings = []

    # å¾ understood æå–æŸ¥è©¢è³‡è¨Š
    region = understood.get("region", state.get("jurisdiction", "æœªçŸ¥"))
    topic = understood.get("topic", "æœªçŸ¥")
    industry = understood.get("industry", "æœªçŸ¥")

    # ===== ç¶å®šå·¥å…·åˆ° LLM =====
    tool_schemas = get_tool_schemas()
    llm_with_tools = llm.bind_tools(tool_schemas)

    # è¼‰å…¥å¤–éƒ¨ prompt
    system_prompt = load_prompt("langgraph_researcher")

    # å¾ understood æå–å…¶ä»–æŸ¥è©¢è³‡è¨Š
    is_follow_up = understood.get("is_follow_up", False)
    time_requirement = understood.get("time_requirement", "none")

    # ===== å¾è³‡æ–™åº«å–å¾—å¿…æœé—œéµå­— =====
    mandatory_keywords = _get_mandatory_keywords_from_db(region, industry, topic)
    mandatory_keywords_text = ""
    if mandatory_keywords:
        print(f"[Researcher] å¾è³‡æ–™åº«è¼‰å…¥ {len(mandatory_keywords)} å€‹å¿…æœé—œéµå­—")
        keyword_lines = []
        for kw in mandatory_keywords:
            keyword_lines.append(f"  - ã€Œ{kw['keyword']}ã€â†’ {kw['regulation_name']}")
        mandatory_keywords_text = f"""

**å¿…æœé—œéµå­—æ¸…å–®**ï¼ˆä¾†è‡ªè³‡æ–™åº«ï¼Œè«‹å„ªå…ˆæœå°‹ï¼‰ï¼š
{chr(10).join(keyword_lines)}
"""

    user_message = f"""
**æŸ¥è©¢è³‡è¨Š**ï¼š
- åŸå§‹æŸ¥è©¢: {state['query']}
- åœ°å€: {region}
- ç”¢æ¥­: {industry}
- ä¸»é¡Œ: {topic}
- æ˜¯å¦ç‚ºè¿½å•: {is_follow_up}
- æ™‚é–“é™åˆ¶: {time_requirement}
{mandatory_keywords_text}
**ä½ çš„ä»»å‹™**ï¼šæ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œåˆ¶å®šä¸¦åŸ·è¡Œæœå°‹ç­–ç•¥ã€‚

è«‹æŒ‰ç…§ Prompt ä¸­çš„æŒ‡å—ï¼š
1. **å„ªå…ˆæœå°‹ä¸Šæ–¹çš„å¿…æœé—œéµå­—**ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
2. è­˜åˆ¥æ‰€æœ‰ç›¸é—œå­é ˜åŸŸ
3. ç‚ºæ¯å€‹å­é ˜åŸŸç”¢ç”Ÿæœå°‹é—œéµå­—
4. åŸ·è¡Œè‡³å°‘ 8-12 æ¬¡æœå°‹
5. ç¢ºä¿ä½¿ç”¨ç•¶åœ°èªè¨€å’Œè‹±æ–‡é—œéµå­—
6. å®Œæˆå¾Œå›è¦†ã€Œæœå°‹å®Œæˆã€

**é‡è¦**: ä¸è¦å¤ªå¿«çµæŸï¼ç¢ºä¿è¦†è“‹æ‰€æœ‰å¯èƒ½ç›¸é—œçš„å­é ˜åŸŸã€‚"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message),
    ]

    # ===== LLM è‡ªä¸»æœå°‹è¿´åœˆ =====
    MAX_SEARCH_ITERATIONS = 15  # æœ€å¤š 15 è¼ªæœå°‹ï¼ˆç·©è§£é¢¨éšªï¼šå¢åŠ æœå°‹æ©Ÿæœƒï¼‰
    total_tool_count = 0

    print(f"[Researcher] é–‹å§‹è‡ªä¸»æœå°‹ï¼ˆåœ°å€: {region}, ç”¢æ¥­: {industry}, ä¸»é¡Œ: {topic}ï¼‰...")

    for iteration in range(MAX_SEARCH_ITERATIONS):
        try:
            response = llm_with_tools.invoke(messages)

            # æª¢æŸ¥æ˜¯å¦æœ‰ tool_calls
            if not response.tool_calls:
                # LLM èªç‚ºæœå°‹å®Œæˆï¼Œæ²’æœ‰æ›´å¤šå·¥å…·èª¿ç”¨
                print(f"[Researcher] LLM æ±ºå®šçµæŸæœå°‹ï¼ˆè¿­ä»£ {iteration + 1}ï¼Œå…± {total_tool_count} æ¬¡å·¥å…·èª¿ç”¨ï¼‰")
                break

            # è™•ç†æ¯å€‹ tool call
            messages.append(response)  # åŠ å…¥ AI çš„å›æ‡‰ï¼ˆåŒ…å« tool_callsï¼‰

            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                total_tool_count += 1

                args_str = json.dumps(tool_args, ensure_ascii=False)[:60]
                print(f"[Researcher] æœå°‹ {total_tool_count}: {tool_name}({args_str}...)")

                # åŸ·è¡Œå·¥å…·
                result = execute_tool_call(tool_call)

                # è§£æçµæœä¸¦åŠ å…¥ findings
                parsed_results = parse_tool_results(result["content"])
                findings.extend(parsed_results)

                # å°‡å·¥å…·çµæœåŠ å…¥å°è©±æ­·å²
                messages.append(ToolMessage(
                    content=result["content"],
                    tool_call_id=tool_call["id"],
                    name=tool_name
                ))

        except Exception as e:
            print(f"[Researcher] è¿­ä»£ {iteration + 1} ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            break

    print(f"[Researcher] æœå°‹å®Œæˆï¼šå…± {total_tool_count} æ¬¡å·¥å…·èª¿ç”¨ï¼Œæ‰¾åˆ° {len(findings)} ç­†çµæœ")

    # ===== å»é‡é‚è¼¯ =====
    seen_urls = set()
    unique_findings = []
    for item in findings:
        url = item.get('url') or item.get('href') or item.get('source_url')
        if url:
            if url not in seen_urls:
                seen_urls.add(url)
                unique_findings.append(item)
        else:
            unique_findings.append(item)

    original_count = len(findings)
    deduplicated_count = len(unique_findings)

    # ===== ä¸¦è¡ŒåŸæ–‡æŠ“å– =====
    TOP_N_TO_FETCH = 50       # æ¥µé™æ¨¡å¼ï¼šæŠ“å–æ›´å¤šåŸæ–‡ï¼ˆå¾ 30 æå‡åˆ° 50ï¼‰
    MAX_CONTENT_CHARS = 10000  # å¢åŠ åŸæ–‡é•·åº¦ä»¥æ•æ‰æ›´å¤šæ³•è¦åç¨±ï¼ˆå¾ 8000 æå‡åˆ° 10000ï¼‰
    MAX_WORKERS = 10          # å¢åŠ ä¸¦è¡Œç·šç¨‹ï¼ˆå¾ 8 æå‡åˆ° 10ï¼‰

    def fetch_single_content(item: dict) -> dict:
        """æŠ“å–å–®ä¸€é …ç›®çš„åŸæ–‡å…§å®¹"""
        url = item.get('url') or item.get('href') or item.get('source_url')
        if not url:
            item['full_content'] = None
            item['content_fetched'] = False
            item['fetch_error'] = 'ç„¡ URL'
            return item

        try:
            is_pdf = (
                url.lower().endswith('.pdf') or
                'pdf' in url.lower() or
                item.get('content_type', '').lower() == 'pdf'
            )

            if is_pdf:
                content_result = fetch_pdf_content(url=url, max_pages=10, max_chars=MAX_CONTENT_CHARS)
            else:
                content_result = fetch_webpage(url=url, extract_text=True)

            content_data = json.loads(content_result) if isinstance(content_result, str) else content_result
            if content_data.get('status') == 'success':
                full_content = content_data.get('content', '')
                if len(full_content) > MAX_CONTENT_CHARS:
                    full_content = full_content[:MAX_CONTENT_CHARS] + '\n... (å…§å®¹å·²æˆªæ–·)'
                item['full_content'] = full_content
                item['content_fetched'] = True
                item['content_type'] = 'pdf' if is_pdf else 'webpage'
            else:
                item['full_content'] = None
                item['content_fetched'] = False
                item['fetch_error'] = content_data.get('error', 'æŠ“å–å¤±æ•—')

        except Exception as e:
            item['full_content'] = None
            item['content_fetched'] = False
            item['fetch_error'] = str(e)

        return item

    items_to_fetch = unique_findings[:TOP_N_TO_FETCH]
    items_no_fetch = unique_findings[TOP_N_TO_FETCH:]

    for item in items_no_fetch:
        item['full_content'] = None
        item['content_fetched'] = False

    print(f"[Researcher] é–‹å§‹ä¸¦è¡ŒæŠ“å– {len(items_to_fetch)} ç­†åŸæ–‡...")
    fetched_items = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_item = {
            executor.submit(fetch_single_content, item.copy()): idx
            for idx, item in enumerate(items_to_fetch)
        }

        for future in as_completed(future_to_item):
            try:
                result_item = future.result(timeout=60)
                fetched_items.append(result_item)
            except Exception as e:
                idx = future_to_item[future]
                item = items_to_fetch[idx].copy()
                item['full_content'] = None
                item['content_fetched'] = False
                item['fetch_error'] = f'æŠ“å–è¶…æ™‚: {str(e)}'
                fetched_items.append(item)

    enriched_findings = fetched_items + items_no_fetch

    fetch_count = sum(1 for item in fetched_items if item.get('content_fetched'))
    pdf_count = sum(1 for item in fetched_items if item.get('content_type') == 'pdf')
    webpage_count = sum(1 for item in fetched_items if item.get('content_type') == 'webpage')

    print(f"[Researcher] ä¸¦è¡ŒæŠ“å–å®Œæˆï¼šæˆåŠŸ {fetch_count}/{len(items_to_fetch)}")

    # ===== é¡¯ç¤ºå¤±æ•—åŸå› çµ±è¨ˆ =====
    failed_items = [item for item in fetched_items if not item.get('content_fetched')]
    if failed_items:
        error_counts = {}
        for item in failed_items:
            error = item.get('fetch_error', 'æœªçŸ¥éŒ¯èª¤')
            # ç°¡åŒ–éŒ¯èª¤è¨Šæ¯ï¼ˆç§»é™¤éé•·çš„ URL æˆ–è©³ç´°è³‡è¨Šï¼‰
            if len(error) > 80:
                error = error[:77] + '...'
            error_counts[error] = error_counts.get(error, 0) + 1

        print(f"[Researcher] æŠ“å–å¤±æ•—åŸå› çµ±è¨ˆ ({len(failed_items)} ç­†):")
        # æŒ‰æ•¸é‡æ’åºï¼Œé¡¯ç¤ºå‰ 5 ç¨®éŒ¯èª¤
        for error, count in sorted(error_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"   - {error}: {count} ç­†")

    state["search_results"] = enriched_findings
    state["status"] = "ready_to_validate"
    state["messages"].append(AIMessage(
        content=f"Researcher è‡ªä¸»æœå°‹èª¿ç”¨ {total_tool_count} æ¬¡å·¥å…·ï¼Œ"
                f"æ‰¾åˆ° {original_count} ç­†çµæœï¼Œå»é‡å¾Œ {deduplicated_count} ç­†ï¼Œ"
                f"æŠ“å– {fetch_count} ç­†åŸæ–‡ï¼ˆPDF: {pdf_count}, ç¶²é : {webpage_count}ï¼‰"
    ))

    return state


def validator_node(state: AgentState) -> AgentState:
    """
    Validator Agent: é©—è­‰æœå°‹çµæœä¸¦ç”Ÿæˆçµæ§‹åŒ–å ±å‘Š
    """
    if state.get("status") != "ready_to_validate":
        return state

    llm = get_llm()

    # å–å¾—åŸå§‹æŸ¥è©¢è³‡è¨Š
    original_query = state.get("query", "")
    jurisdiction = state.get("jurisdiction", "è‡ªå‹•åµæ¸¬")

    # è¼‰å…¥å¤–éƒ¨ prompt
    system_prompt = load_prompt("langgraph_validator")

    # ===== å‹•æ…‹èª¿æ•´ï¼šæ ¹æ“šåŸæ–‡é•·åº¦è‡ªå‹•èª¿æ•´æ•¸é‡ =====
    # GPT-5.1: 400K context (272K input, 128K output)
    # ä¸­æ–‡ç´„ 1.5-2 token/å­—å…ƒï¼Œè¨­å®š 150,000 å­—å…ƒç´„ 225K-300K tokens
    TARGET_TOTAL_CHARS = 150000
    MAX_CONTENT_LENGTH = 2000     # æ¯ç­†åŸæ–‡ 2000 å­—ï¼ˆå……åˆ†æ•æ‰æ³•è¦å…§å®¹ï¼‰

    search_results = state.get('search_results', [])

    # å„ªå…ˆè™•ç†æœ‰åŸæ–‡çš„çµæœ
    results_with_content = [r for r in search_results if r.get('content_fetched')]
    results_without_content = [r for r in search_results if not r.get('content_fetched')]

    # ===== å‹•æ…‹ç²¾ç°¡ï¼šç¢ºä¿ç¸½å­—å…ƒæ•¸ä¸è¶…éé™åˆ¶ =====
    trimmed_results = []
    total_chars = 0

    # Phase 1: å„ªå…ˆåŠ å…¥æœ‰åŸæ–‡çš„çµæœ
    for r in results_with_content:
        trimmed = {
            'title': r.get('title') or r.get('name') or 'æœªçŸ¥',
            'url': r.get('url') or r.get('href') or '',
            'snippet': (r.get('snippet') or r.get('body') or '')[:300],  # é™åˆ¶ snippet é•·åº¦
            'content_type': r.get('content_type', 'unknown'),
        }
        # æˆªæ–· full_content
        full_content = r.get('full_content', '')
        if full_content and len(full_content) > MAX_CONTENT_LENGTH:
            trimmed['full_content'] = full_content[:MAX_CONTENT_LENGTH] + '\n... (å…§å®¹å·²æˆªæ–·)'
        else:
            trimmed['full_content'] = full_content or ''

        # è¨ˆç®—æ­¤ç­†è³‡æ–™çš„ä¼°è¨ˆå­—å…ƒæ•¸
        item_chars = len(json.dumps(trimmed, ensure_ascii=False))

        # æª¢æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
        if total_chars + item_chars > TARGET_TOTAL_CHARS:
            print(f"[Validator] é”åˆ°å­—å…ƒé™åˆ¶ï¼Œåœæ­¢æ–¼ {len(trimmed_results)} ç­†æœ‰åŸæ–‡çµæœ")
            break

        trimmed_results.append(trimmed)
        total_chars += item_chars

    # Phase 2: å¦‚æœé‚„æœ‰ç©ºé–“ï¼ŒåŠ å…¥ç„¡åŸæ–‡çš„çµæœï¼ˆè¼ƒå°çš„è³‡æ–™é‡ï¼‰
    for r in results_without_content:
        trimmed = {
            'title': r.get('title') or r.get('name') or 'æœªçŸ¥',
            'url': r.get('url') or r.get('href') or '',
            'snippet': (r.get('snippet') or r.get('body') or '')[:200],
            'full_content': None,
        }

        item_chars = len(json.dumps(trimmed, ensure_ascii=False))

        if total_chars + item_chars > TARGET_TOTAL_CHARS:
            break

        trimmed_results.append(trimmed)
        total_chars += item_chars

    print(f"[Validator] ç²¾ç°¡å¾Œï¼š{len(trimmed_results)} ç­†ï¼Œç¸½è¨ˆç´„ {total_chars:,} å­—å…ƒ")

    user_message = f"""
**åŸå§‹æŸ¥è©¢**: {original_query}
**ç›®æ¨™åœ°å€**: {jurisdiction}

**æœå°‹çµæœçµ±è¨ˆ**:
- ç¸½çµæœæ•¸: {len(search_results)}
- æˆåŠŸæŠ“å–åŸæ–‡: {len(results_with_content)} ç­†
- æœ¬æ¬¡é©—è­‰: {len(trimmed_results)} ç­†ï¼ˆå·²ç²¾ç°¡ï¼‰

**æœå°‹çµæœï¼ˆå·²ç²¾ç°¡ï¼‰**:
{json.dumps(trimmed_results, ensure_ascii=False, indent=2)}

è«‹æ ¹æ“šä»¥ä¸Šè³‡æ–™ï¼š
1. éæ¿¾ä¸ç›¸é—œçš„çµæœ
2. å¾æœ‰åŸæ–‡çš„çµæœä¸­æå–é‡è¦æ¢æ–‡
3. æ•´ç†æ™‚é–“è»¸
4. ç”Ÿæˆåˆè¦æª¢æ ¸æ¸…å–®
5. æä¾›æ•´é«”æ‘˜è¦

**é‡è¦**ï¼šè«‹ç¢ºä¿è¼¸å‡ºç‚ºæœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message),
    ]

    # ===== å¢åŠ é‡è©¦æ©Ÿåˆ¶ =====
    MAX_RETRIES = 3
    last_error = None

    for attempt in range(MAX_RETRIES):
        try:
            response = llm.invoke(messages)
            content = response.content

            # ===== è¨ºæ–·ï¼šæª¢æ¸¬ç©ºå›æ‡‰ =====
            if not content or not content.strip():
                print(f"[Validator] è­¦å‘Šï¼šLLM è¿”å›ç©ºå…§å®¹ (attempt {attempt + 1})")
                raise ValueError("LLM è¿”å›ç©ºå…§å®¹ï¼Œå¯èƒ½æ˜¯è¼¸å…¥éé•·æˆ–æ¨¡å‹é™åˆ¶")

            print(f"[Validator] LLM å›æ‡‰é•·åº¦: {len(content)} å­—å…ƒ (attempt {attempt + 1})")

            # è§£æ JSON
            if "```json" in content:
                json_str = content.split("```json")[1].split("```")[0]
                validation = json.loads(json_str)
            elif "```" in content:
                # å˜—è©¦æå–ä»»ä½• code block
                json_str = content.split("```")[1]
                if json_str.startswith("json"):
                    json_str = json_str[4:]
                validation = json.loads(json_str)
            else:
                validation = json.loads(content)

            # é©—è­‰å¿…è¦æ¬„ä½å­˜åœ¨
            if 'verified_regulations' not in validation:
                validation['verified_regulations'] = []
            if 'timeline' not in validation:
                validation['timeline'] = []
            if 'compliance_checklist' not in validation:
                validation['compliance_checklist'] = []

            # ç¢ºä¿å…è²¬è²æ˜å­˜åœ¨
            if 'disclaimer' not in validation:
                validation['disclaimer'] = {
                    "zh": "æœ¬æŸ¥è©¢çµæœåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ³•å¾‹æ„è¦‹ã€‚æ³•è¦å…§å®¹å¯èƒ½éš¨æ™‚æ›´æ–°ï¼Œ"
                          "è«‹ä»¥å„ä¸»ç®¡æ©Ÿé—œå…¬å‘Šä¹‹æœ€æ–°ç‰ˆæœ¬ç‚ºæº–ã€‚ä½¿ç”¨è€…æ‡‰è‡ªè¡Œè«®è©¢å°ˆæ¥­æ³•å¾‹äººå“¡ä»¥ç¢ºèªé©ç”¨æ€§ã€‚",
                    "en": "This query result is for reference only and does not constitute legal advice. "
                          "Regulatory content may be updated at any time. Please refer to the latest version "
                          "published by the relevant authorities. Users should consult qualified legal "
                          "professionals to confirm applicability."
                }

            # æˆåŠŸï¼šå°‡å®Œæ•´çš„é©—è­‰çµæœå­˜å…¥ state
            state["validated_results"] = validation
            state["status"] = "completed"
            state["messages"].append(AIMessage(
                content=f"Validator é©—è­‰å®Œæˆ: {validation.get('validation_result', 'unknown')}ï¼Œ"
                        f"è­˜åˆ¥ {len(validation.get('verified_regulations', []))} é …ç›¸é—œæ³•è¦ï¼Œ"
                        f"ç”Ÿæˆ {len(validation.get('compliance_checklist', []))} é …æª¢æ ¸é …ç›®"
            ))
            return state

        except json.JSONDecodeError as e:
            last_error = e
            print(f"[Validator] JSON è§£æå¤±æ•—: {e} (attempt {attempt + 1})")
            # JSON è§£æå¤±æ•—ï¼Œå¦‚æœé‚„æœ‰é‡è©¦æ¬¡æ•¸ï¼Œæ·»åŠ æç¤ºå¾Œé‡è©¦
            if attempt < MAX_RETRIES - 1:
                messages.append(AIMessage(content=content if content else ""))
                messages.append(HumanMessage(
                    content="ä¸Šè¿°å›æ‡‰ç„¡æ³•è§£æç‚º JSONã€‚è«‹é‡æ–°è¼¸å‡ºï¼Œç¢ºä¿æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼ˆä»¥ ```json é–‹é ­ï¼Œ``` çµå°¾ï¼‰ã€‚"
                ))
            continue

        except ValueError as e:
            # ç©ºå›æ‡‰éŒ¯èª¤ï¼Œé‡è©¦
            last_error = e
            print(f"[Validator] ValueError: {e} (attempt {attempt + 1})")
            if attempt < MAX_RETRIES - 1:
                continue
            break

        except Exception as e:
            last_error = e
            print(f"[Validator] æœªçŸ¥éŒ¯èª¤: {type(e).__name__}: {e}")
            break

    # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œå»ºæ§‹åŸºæœ¬çµæ§‹
    state["validated_results"] = {
        "validation_result": "error",
        "summary": f"é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(last_error)}",
        "verified_regulations": trimmed_results,  # ä½¿ç”¨ç²¾ç°¡å¾Œçš„çµæœ
        "timeline": [],
        "compliance_checklist": [],
        "warnings": [f"é©—è­‰éŒ¯èª¤: {str(last_error)}"],
        "limitations": ["ç„¡æ³•å®Œæˆå®Œæ•´é©—è­‰ï¼Œå·²è¿”å›åŸå§‹æœå°‹çµæœ"],
        "confidence_score": 0.3,
        "disclaimer": {
            "zh": "æœ¬æŸ¥è©¢çµæœåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ³•å¾‹æ„è¦‹ã€‚æ³•è¦å…§å®¹å¯èƒ½éš¨æ™‚æ›´æ–°ï¼Œ"
                  "è«‹ä»¥å„ä¸»ç®¡æ©Ÿé—œå…¬å‘Šä¹‹æœ€æ–°ç‰ˆæœ¬ç‚ºæº–ã€‚ä½¿ç”¨è€…æ‡‰è‡ªè¡Œè«®è©¢å°ˆæ¥­æ³•å¾‹äººå“¡ä»¥ç¢ºèªé©ç”¨æ€§ã€‚",
            "en": "This query result is for reference only and does not constitute legal advice. "
                  "Regulatory content may be updated at any time. Please refer to the latest version "
                  "published by the relevant authorities. Users should consult qualified legal "
                  "professionals to confirm applicability."
        }
    }
    state["status"] = "completed"
    state["messages"].append(AIMessage(content=f"Validator è­¦å‘Š: {str(last_error)}"))

    return state


# ===== Conditional Edge =====
def should_continue(state: AgentState) -> Literal["researcher", "end"]:
    """æ±ºå®šä¸‹ä¸€æ­¥ï¼šç¹¼çºŒæœå°‹æˆ–çµæŸ"""
    if state.get("clarification_needed"):
        return "end"
    if state.get("error"):
        return "end"
    if state.get("status") == "ready_to_search":
        return "researcher"
    return "end"


# ===== Build Graph =====
def create_regulation_graph():
    """å‰µå»ºæ³•è¦æŸ¥è©¢ workflow graph"""
    workflow = StateGraph(AgentState)

    # æ·»åŠ  nodes
    workflow.add_node("planner", planner_node)
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("validator", validator_node)

    # è¨­å®šæµç¨‹
    workflow.set_entry_point("planner")
    workflow.add_conditional_edges(
        "planner",
        should_continue,
        {
            "researcher": "researcher",
            "end": END,
        }
    )
    workflow.add_edge("researcher", "validator")
    workflow.add_edge("validator", END)

    return workflow.compile()


# ===== Main Query Handler =====
class RegulationAgentTeam:
    """
    æ³•è¦æŸ¥è©¢ Agent åœ˜éšŠ (LangGraph ç‰ˆæœ¬)
    """

    def __init__(self, status_callback: Optional[Callable[[str], None]] = None):
        """
        åˆå§‹åŒ– Agent åœ˜éšŠ

        Args:
            status_callback: ç‹€æ…‹æ›´æ–°å›èª¿å‡½æ•¸
        """
        self.status_callback = status_callback or (lambda x: None)
        self.graph = create_regulation_graph()

        # åˆå§‹åŒ–å¿«å–
        from ..utils.cache import get_cache
        self.cache = get_cache()

        # é€²åº¦è¿½è¹¤
        self._progress_messages = []

        self._report_status("LangGraph Agent åœ˜éšŠåˆå§‹åŒ–å®Œæˆ")

    def get_progress_messages(self) -> list:
        """å–å¾—ä¸¦æ¸…ç©ºé€²åº¦è¨Šæ¯"""
        msgs = self._progress_messages.copy()
        self._progress_messages.clear()
        return msgs

    def add_progress(self, message: str):
        """æ–°å¢é€²åº¦è¨Šæ¯"""
        self._progress_messages.append(message)

    def _report_status(self, message: str):
        """å ±å‘Šç‹€æ…‹"""
        self.status_callback(message)

    def process_query(
        self,
        query: str,
        jurisdiction: str = "è‡ªå‹•åµæ¸¬",
        skip_cache: bool = False,
        conversation_history: str = "",
        previous_results_summary: str = "",
    ) -> Generator[tuple[str, Optional[dict]], None, None]:
        """
        è™•ç†æ³•è¦æŸ¥è©¢

        Args:
            query: ä½¿ç”¨è€…æŸ¥è©¢
            jurisdiction: ç›®æ¨™åœ°å€
            skip_cache: æ˜¯å¦è·³éå¿«å–ï¼ˆå¼·åˆ¶é‡æ–°æŸ¥è©¢ï¼‰
            conversation_history: æ ¼å¼åŒ–çš„å°è©±æ­·å²ï¼ˆå¤šè¼ªå°è©±æ”¯æ´ï¼‰
            previous_results_summary: ä¸Šæ¬¡æŸ¥è©¢çµæœæ‘˜è¦ï¼ˆç”¨æ–¼è¿½å•ï¼‰

        Yields:
            (ç‹€æ…‹è¨Šæ¯, çµæœè³‡æ–™)
        """
        yield ("ğŸš€ å•Ÿå‹• LangGraph Agent åœ˜éšŠ...", None)

        # ===== æå–åŸå§‹æŸ¥è©¢ï¼ˆç”¨æ–¼å¿«å– keyï¼‰=====
        # å¦‚æœæŸ¥è©¢åŒ…å«ã€Œç”¨æˆ¶è£œå……èªªæ˜ã€ï¼Œåªä½¿ç”¨ç¬¬ä¸€éƒ¨åˆ†ä½œç‚ºå¿«å– key
        cache_key_query = query.split("\n\nã€ç”¨æˆ¶è£œå……èªªæ˜ã€‘")[0].strip()

        # ===== æª¢æŸ¥å¿«å– =====
        if not skip_cache:
            cached_result = self.cache.get(cache_key_query, jurisdiction)
            if cached_result:
                print("[Cache] å¿«å–å‘½ä¸­ï¼Œç›´æ¥è¿”å›çµæœ")
                yield ("ğŸ“¦ å¾å¿«å–è¼‰å…¥çµæœ...", None)
                cached_result['from_cache'] = True
                yield ("ğŸ‰ æŸ¥è©¢å®Œæˆï¼ˆå¿«å–ï¼‰!", cached_result)
                return

        # åˆå§‹åŒ–ç‹€æ…‹ï¼ˆå«å°è©±æ­·å²ï¼‰
        initial_state: AgentState = {
            "messages": [],
            "query": query,
            "jurisdiction": jurisdiction,
            "conversation_history": conversation_history,
            "previous_results_summary": previous_results_summary,
            "planner_analysis": {},
            "clarification_needed": False,
            "questions": [],
            "search_results": [],
            "validated_results": [],
            "status": "starting",
            "error": None,
        }

        try:
            # ===== ä½¿ç”¨ stream() å–ä»£ invoke() ä»¥ç²å–å³æ™‚é€²åº¦ =====
            final_state = None
            current_node = None

            # é å…ˆé¡¯ç¤º Planner é€²åº¦ï¼ˆå› ç‚º stream() åœ¨ node å®Œæˆå¾Œæ‰è¿”å›ï¼‰
            yield ("ğŸ“‹ Planner æ­£åœ¨åˆ†ææŸ¥è©¢æ„åœ–...", None)
            yield ("   â”œâ”€ è­˜åˆ¥ç›®æ¨™åœ°å€èˆ‡æ³•è¦é¡å‹...", None)
            yield ("   â””â”€ è¦åŠƒå¤šé—œéµå­—æœå°‹ç­–ç•¥...", None)

            for event in self.graph.stream(initial_state):
                # event æ ¼å¼: {node_name: state}
                for node_name, state in event.items():
                    if node_name != current_node:
                        # åœ¨é€²å…¥ä¸‹ä¸€å€‹ node å‰é¡¯ç¤ºé€²åº¦
                        if current_node == "planner" and node_name == "researcher":
                            # Planner å®Œæˆï¼Œé¡¯ç¤ºåˆ†æçµæœæ‘˜è¦
                            plan = state.get("planner_analysis", {})
                            search_plan = plan.get("search_plan", [])
                            understood = plan.get("understood", {})
                            region = understood.get("region", "æœªçŸ¥")
                            topic = understood.get("topic", "æœªçŸ¥")
                            yield (f"   âœ“ åˆ†æå®Œæˆï¼š{region} - {topic}", None)

                            # é¡¯ç¤º Researcher é€²åº¦
                            yield ("ğŸ” Researcher æ­£åœ¨åŸ·è¡Œæœå°‹...", None)
                            if search_plan:
                                yield (f"   â”œâ”€ åŸ·è¡Œ {len(search_plan)} å€‹å¤šé—œéµå­—æœå°‹...", None)
                            yield ("   â”œâ”€ ä¸¦è¡ŒæŠ“å–ç¶²é åŸæ–‡ï¼ˆæœ€å¤š 15 ç­†ï¼‰...", None)
                            yield ("   â””â”€ åŸ·è¡Œå»é‡èˆ‡è³‡æ–™æ¸…æ´—...", None)

                        elif current_node == "researcher" and node_name == "validator":
                            # Researcher å®Œæˆï¼Œé¡¯ç¤ºçµæœæ‘˜è¦
                            search_results = state.get("search_results", [])
                            fetched_count = sum(1 for r in search_results if r.get("content_fetched"))
                            yield (f"   âœ“ æœå°‹å®Œæˆï¼šæ‰¾åˆ° {len(search_results)} ç­†ï¼ˆæŠ“å– {fetched_count} ç¯‡åŸæ–‡ï¼‰", None)

                            # é¡¯ç¤º Validator é€²åº¦
                            yield ("âœ… Validator æ­£åœ¨é©—è­‰çµæœ...", None)
                            yield ("   â”œâ”€ ç¯©é¸ç›¸é—œæ³•è¦...", None)
                            yield ("   â”œâ”€ æå–é‡è¦æ¢æ–‡...", None)
                            yield ("   â””â”€ ç”Ÿæˆåˆè¦æª¢æ ¸æ¸…å–®...", None)

                        current_node = node_name

                    final_state = state

            # Validator å®Œæˆæ‘˜è¦
            if final_state and not final_state.get("clarification_needed"):
                validated = final_state.get("validated_results", {})
                if isinstance(validated, dict):
                    reg_count = len(validated.get("verified_regulations", []))
                    checklist_count = len(validated.get("compliance_checklist", []))
                    yield (f"   âœ“ é©—è­‰å®Œæˆï¼š{reg_count} é …æ³•è¦ã€{checklist_count} é …æª¢æ ¸æ¸…å–®", None)

            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
            if final_state and final_state.get("clarification_needed"):
                yield ("â¸ï¸ éœ€è¦æ¾„æ¸…æŸ¥è©¢æ„åœ–", {
                    "status": "needs_clarification",
                    "query": query,
                    "questions": final_state.get("questions", []),
                    "analysis": final_state.get("planner_analysis", {}),
                    "timestamp": datetime.now().isoformat(),
                })
                return

            if not final_state:
                raise ValueError("Graph åŸ·è¡Œæœªè¿”å›çµæœ")

            # æ§‹å»ºæœ€çµ‚çµæœ
            result = {
                "status": "success",
                "query": query,
                "original_query": cache_key_query,  # å„²å­˜åŸå§‹æŸ¥è©¢ï¼ˆä¸å«è£œå……èªªæ˜ï¼‰
                "model_used": "LangGraph Multi-Agent Team",
                "regulations": final_state.get("validated_results", []),
                "notes": "æŸ¥è©¢å®Œæˆ",
                "timestamp": datetime.now().isoformat(),
                "from_cache": False,
            }

            # ===== å„²å­˜åˆ°å¿«å–ï¼ˆä½¿ç”¨åŸå§‹æŸ¥è©¢ä½œç‚º keyï¼‰=====
            cache_id = self.cache.set(cache_key_query, jurisdiction, result)
            print(f"[Cache] çµæœå·²å„²å­˜ï¼Œkey: '{cache_key_query[:30]}...', ID: {cache_id}")

            yield ("ğŸ‰ æŸ¥è©¢å®Œæˆ!", result)

        except Exception as e:
            yield (f"âŒ è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}", {
                "status": "error",
                "error": str(e),
            })


# ===== ä¾¿æ·å‡½æ•¸ =====
_team_instance: Optional[RegulationAgentTeam] = None


def get_agent_team(status_callback: Optional[Callable[[str], None]] = None) -> RegulationAgentTeam:
    """å–å¾—å…¨åŸŸ Agent åœ˜éšŠå¯¦ä¾‹"""
    global _team_instance
    if _team_instance is None:
        _team_instance = RegulationAgentTeam(status_callback)
    return _team_instance


def reset_agent_team():
    """é‡ç½® Agent åœ˜éšŠ"""
    global _team_instance
    _team_instance = None
